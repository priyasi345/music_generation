{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled313.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+zdPLXRj1sJwxpB6+wCzb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyasi345/music_generation/blob/master/midi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C1RREzeij60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Generate_Music_with_Transofrmers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcAhWEM0gErX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8bf22d9b-941b-44c8-d33c-a00445c9dd56"
      },
      "source": [
        "!git clone https://github.com/Skuldur/Classical-Piano-Composer.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Classical-Piano-Composer' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr-c1G9dgOWd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "dc32920f-b44d-4340-b485-a681647e55c0"
      },
      "source": [
        "!mv Classical-Piano-Composer/midi_songs midi_songs\n",
        "!mv Classical-Piano-Composer/data data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'Classical-Piano-Composer/midi_songs': No such file or directory\n",
            "mv: cannot stat 'Classical-Piano-Composer/data': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJSivb7ngPVV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "outputId": "d367fbee-5793-4cfb-eabc-a5f154e4f7a4"
      },
      "source": [
        "!ls -GFlash --color"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 30M\n",
            " 4.0K drwxr-xr-x 1 root  4.0K Jun 27 02:50 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            " 4.0K drwxr-xr-x 1 root  4.0K Jun 27 02:30 \u001b[01;34m..\u001b[0m/\n",
            " 4.0K drwxr-xr-x 4 root  4.0K Jun 27 02:32 \u001b[01;34mClassical-Piano-Composer\u001b[0m/\n",
            " 4.0K drwxr-xr-x 1 root  4.0K Jun 25 17:02 \u001b[01;34m.config\u001b[0m/\n",
            " 4.0K drwxr-xr-x 2 root  4.0K Jun 27 02:32 \u001b[01;34mdata\u001b[0m/\n",
            " 4.0K drwxr-xr-x 2 root  4.0K Jun 27 02:32 \u001b[01;34mmidi_songs\u001b[0m/\n",
            " 4.0K drwxr-xr-x 1 root  4.0K Jun 17 16:18 \u001b[01;34msample_data\u001b[0m/\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:34 weights-improvement-01-5.2320-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:34 weights-improvement-02-4.9537-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:35 weights-improvement-03-4.7792-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:36 weights-improvement-04-4.6602-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:36 weights-improvement-05-4.5525-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:37 weights-improvement-06-4.4925-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:37 weights-improvement-07-4.4166-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:38 weights-improvement-08-4.3722-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:38 weights-improvement-09-4.3306-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:39 weights-improvement-10-4.2813-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:40 weights-improvement-11-4.2428-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:40 weights-improvement-12-4.2134-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:41 weights-improvement-13-4.1811-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:41 weights-improvement-14-4.1571-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:42 weights-improvement-15-4.1335-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:42 weights-improvement-16-4.1102-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:43 weights-improvement-17-4.1017-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:43 weights-improvement-18-4.0826-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:44 weights-improvement-19-4.0732-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:45 weights-improvement-20-4.0550-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:45 weights-improvement-21-4.0293-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:46 weights-improvement-22-4.0285-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:46 weights-improvement-23-4.0084-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:47 weights-improvement-24-4.0059-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:48 weights-improvement-25-3.9873-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:48 weights-improvement-26-3.9748-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:49 weights-improvement-27-3.9657-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:49 weights-improvement-28-3.9501-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:50 weights-improvement-29-3.9399-bigger.hdf5\n",
            "1016K -rw-r--r-- 1 root 1014K Jun 27 02:50 weights-improvement-30-3.9192-bigger.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xitCj_vDgSXr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "35f27693-85d0-4252-8929-1e285ab9f697"
      },
      "source": [
        "import os\n",
        "files = os.listdir('midi_songs/')\n",
        "file_path = [os.path.join('midi_songs/',i) for i in files]\n",
        "print(f\"Last 10 file paths: {file_path[-10:]}\")\n",
        "print(f\"Number of files: {len(file_path)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Last 10 file paths: ['midi_songs/thoughts.mid', 'midi_songs/goldsaucer.mid', 'midi_songs/redwings.mid', 'midi_songs/ff4-fight1.mid', 'midi_songs/ff1battp.mid', 'midi_songs/relmstheme-piano.mid', 'midi_songs/Kingdom_Hearts_Traverse_Town.mid', 'midi_songs/lurk_in_dark.mid', 'midi_songs/Finalfantasy6fanfarecomplete.mid', 'midi_songs/Eternal_Harvest.mid']\n",
            "Number of files: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pxkTVQ-gU08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# try different values of retaining the amount data\n",
        "# remove the files\n",
        "for f in file_path[10:]:\n",
        "  os.remove(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4hBN5AuhNYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing necessary modules\n",
        "from __future__ import absolute_import, print_function, unicode_literals, division\n",
        "import os\n",
        "import gc\n",
        "gc.enable()\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "import glob\n",
        "import pickle\n",
        "import numpy as np\n",
        "from music21 import converter, instrument, note, chord \n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Dropout, BatchNormalization as BatchNorm, LSTM, Activation \n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
        "from keras.utils import plot_model, np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMTdAd9chQpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_network():\n",
        "  notes = get_notes()\n",
        "  n_vocab = len(set(notes))\n",
        "  network_input, network_output = prepare_sequences(notes, n_vocab)\n",
        "  model = create_network(network_input, n_vocab)\n",
        "  train(model, network_input, network_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpbxfQt2hUtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_notes():\n",
        "  notes = []\n",
        "  for file in glob.glob(\"midi_songs/*.mid\"):\n",
        "    midi = converter.parse(file)\n",
        "    print(f\"Parsing {file}.\")\n",
        "    notes_to_parse = None\n",
        "    try:\n",
        "      s2 = instrument.partitionByInstrument(midi)\n",
        "      notes_to_parse = s2.parts[0].recurse()\n",
        "    except:\n",
        "      notes_to_parse = midi.flat.notes \n",
        "    for element in notes_to_parse:\n",
        "      if isinstance(element, note.Note):\n",
        "        notes.append(str(element.pitch))\n",
        "      elif isinstance(element, chord.Chord):\n",
        "        notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "  with open('data/notes','wb') as filepath:\n",
        "    pickle.dump(notes, filepath)\n",
        "  return notes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j7LkN7_hWSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sequences(notes, n_vocab):\n",
        "  sequence_length = 1000\n",
        "  pitchnames = sorted(set(item for item in notes))\n",
        "  note_to_int = dict((note, number) for number, note in enumerate(pitchnames)) \n",
        "  network_input = []\n",
        "  network_output = []\n",
        "\n",
        "  for i in range(0, len(notes)-sequence_length, 1):\n",
        "    sequence_in = notes[i:i+sequence_length]\n",
        "    sequence_out = notes[i+sequence_length]\n",
        "    network_input.append([note_to_int[char] for char in sequence_in])\n",
        "    network_output.append(note_to_int[sequence_out])\n",
        "  n_patterns = len(network_input)\n",
        "  network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "  network_input = network_input / float(n_vocab)\n",
        "  network_output = np_utils.to_categorical(network_output)\n",
        "  return (network_input, network_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aufBZhKnheTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_network(network_input, n_vocab):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(\n",
        "      128,\n",
        "      input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "      recurrent_dropout=0.3,\n",
        "      return_sequences=True\n",
        "  ))\n",
        "  model.add(LSTM(64, recurrent_dropout=0.3))\n",
        "  model.add(BatchNorm())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(32))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNorm())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(n_vocab))\n",
        "  model.add(Activation('softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',optimizer='rmsprop')\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BpNUtS5hjQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, network_input, network_output):\n",
        "  filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(\n",
        "      filepath,\n",
        "      monitor='loss',\n",
        "      verbose=0,\n",
        "      save_best_only=True,\n",
        "      mode=\"min\"\n",
        "  )\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='loss', verbose=1, patience=4, mode='min', min_lr=0.000001, factor=0.3)\n",
        "  early_stopping = EarlyStopping(monitor=\"loss\",verbose=1, patience=6, mode='min')\n",
        "  callbacks_list = [checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "  model.fit(network_input, network_output, epochs=200, batch_size=512, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBnHqceknX2-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "20c65e04-fba5-4ae1-8ec7-cf7b33869c31"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  train_network()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing midi_songs/thoughts.mid.\n",
            "Parsing midi_songs/goldsaucer.mid.\n",
            "Parsing midi_songs/redwings.mid.\n",
            "Parsing midi_songs/ff4-fight1.mid.\n",
            "Parsing midi_songs/ff1battp.mid.\n",
            "Parsing midi_songs/relmstheme-piano.mid.\n",
            "Parsing midi_songs/Kingdom_Hearts_Traverse_Town.mid.\n",
            "Parsing midi_songs/lurk_in_dark.mid.\n",
            "Parsing midi_songs/Finalfantasy6fanfarecomplete.mid.\n",
            "Parsing midi_songs/Eternal_Harvest.mid.\n",
            "Epoch 1/200\n",
            "5348/5348 [==============================] - 36s 7ms/step - loss: 5.2433\n",
            "Epoch 2/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 5.0027\n",
            "Epoch 3/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.8426\n",
            "Epoch 4/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.6908\n",
            "Epoch 5/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.5996\n",
            "Epoch 6/200\n",
            "5348/5348 [==============================] - 35s 6ms/step - loss: 4.4973\n",
            "Epoch 7/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.4226\n",
            "Epoch 8/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.3787\n",
            "Epoch 9/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.3335\n",
            "Epoch 10/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.2886\n",
            "Epoch 11/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.2564\n",
            "Epoch 12/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.2164\n",
            "Epoch 13/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.1876\n",
            "Epoch 14/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.1658\n",
            "Epoch 15/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.1411\n",
            "Epoch 16/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.1201\n",
            "Epoch 17/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.1022\n",
            "Epoch 18/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.0847\n",
            "Epoch 19/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.0571\n",
            "Epoch 20/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.0459\n",
            "Epoch 21/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 4.0378\n",
            "Epoch 22/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.0275\n",
            "Epoch 23/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 4.0104\n",
            "Epoch 24/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.9864\n",
            "Epoch 25/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.9627\n",
            "Epoch 26/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.9463\n",
            "Epoch 27/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.9365\n",
            "Epoch 28/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.9290\n",
            "Epoch 29/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.9354\n",
            "Epoch 30/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.9011\n",
            "Epoch 31/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.8988\n",
            "Epoch 32/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.8888\n",
            "Epoch 33/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.8637\n",
            "Epoch 34/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.8674\n",
            "Epoch 35/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.8692\n",
            "Epoch 36/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.8577\n",
            "Epoch 37/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.8462\n",
            "Epoch 38/200\n",
            "5348/5348 [==============================] - 35s 6ms/step - loss: 3.8221\n",
            "Epoch 39/200\n",
            "5348/5348 [==============================] - 36s 7ms/step - loss: 3.8299\n",
            "Epoch 40/200\n",
            "5348/5348 [==============================] - 36s 7ms/step - loss: 3.8224\n",
            "Epoch 41/200\n",
            "5348/5348 [==============================] - 35s 7ms/step - loss: 3.8236\n",
            "Epoch 42/200\n",
            "5348/5348 [==============================] - 35s 7ms/step - loss: 3.8191\n",
            "Epoch 43/200\n",
            "5348/5348 [==============================] - 35s 7ms/step - loss: 3.8009\n",
            "Epoch 44/200\n",
            "5348/5348 [==============================] - 35s 7ms/step - loss: 3.7855\n",
            "Epoch 45/200\n",
            "5348/5348 [==============================] - 35s 7ms/step - loss: 3.7791\n",
            "Epoch 46/200\n",
            "5348/5348 [==============================] - 36s 7ms/step - loss: 3.7953\n",
            "Epoch 47/200\n",
            "5348/5348 [==============================] - 36s 7ms/step - loss: 3.7698\n",
            "Epoch 48/200\n",
            "5348/5348 [==============================] - 35s 7ms/step - loss: 3.7724\n",
            "Epoch 49/200\n",
            "5348/5348 [==============================] - 35s 7ms/step - loss: 3.7400\n",
            "Epoch 50/200\n",
            "5348/5348 [==============================] - 35s 7ms/step - loss: 3.7392\n",
            "Epoch 51/200\n",
            "5348/5348 [==============================] - 35s 7ms/step - loss: 3.7426\n",
            "Epoch 52/200\n",
            "5348/5348 [==============================] - 35s 7ms/step - loss: 3.7548\n",
            "Epoch 53/200\n",
            "5348/5348 [==============================] - 35s 6ms/step - loss: 3.7366\n",
            "Epoch 54/200\n",
            "5348/5348 [==============================] - 35s 6ms/step - loss: 3.7298\n",
            "Epoch 55/200\n",
            "5348/5348 [==============================] - 35s 7ms/step - loss: 3.7251\n",
            "Epoch 56/200\n",
            "5348/5348 [==============================] - 35s 7ms/step - loss: 3.7252\n",
            "Epoch 57/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.7088\n",
            "Epoch 58/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.6997\n",
            "Epoch 59/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.7096\n",
            "Epoch 60/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.7000\n",
            "Epoch 61/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.7101\n",
            "Epoch 62/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.6841\n",
            "Epoch 63/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.6824\n",
            "Epoch 64/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.6779\n",
            "Epoch 65/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.6526\n",
            "Epoch 66/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.6548\n",
            "Epoch 67/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.6699\n",
            "Epoch 68/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.6419\n",
            "Epoch 69/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.6449\n",
            "Epoch 70/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.6444\n",
            "Epoch 71/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.6447\n",
            "Epoch 72/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.6295\n",
            "Epoch 73/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.6334\n",
            "Epoch 74/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.6259\n",
            "Epoch 75/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.6297\n",
            "Epoch 76/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.6050\n",
            "Epoch 77/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.6138\n",
            "Epoch 78/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.5970\n",
            "Epoch 79/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.5908\n",
            "Epoch 80/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.6029\n",
            "Epoch 81/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.5835\n",
            "Epoch 82/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.5831\n",
            "Epoch 83/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.5815\n",
            "Epoch 84/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.5717\n",
            "Epoch 85/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.5717\n",
            "Epoch 86/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.5655\n",
            "Epoch 87/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.5615\n",
            "Epoch 88/200\n",
            "5348/5348 [==============================] - 35s 6ms/step - loss: 3.5453\n",
            "Epoch 89/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.5430\n",
            "Epoch 90/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.5449\n",
            "Epoch 91/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.5370\n",
            "Epoch 92/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.5258\n",
            "Epoch 93/200\n",
            "5348/5348 [==============================] - 35s 6ms/step - loss: 3.5290\n",
            "Epoch 94/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.5161\n",
            "Epoch 95/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4995\n",
            "Epoch 96/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.5129\n",
            "Epoch 97/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.5013\n",
            "Epoch 98/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4923\n",
            "Epoch 99/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4987\n",
            "Epoch 100/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4932\n",
            "Epoch 101/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4841\n",
            "Epoch 102/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4762\n",
            "Epoch 103/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.5342\n",
            "Epoch 104/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4904\n",
            "Epoch 105/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4609\n",
            "Epoch 106/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4628\n",
            "Epoch 107/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4648\n",
            "Epoch 108/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4573\n",
            "Epoch 109/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4377\n",
            "Epoch 110/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4454\n",
            "Epoch 111/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4281\n",
            "Epoch 112/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4410\n",
            "Epoch 113/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4224\n",
            "Epoch 114/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4195\n",
            "Epoch 115/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4283\n",
            "Epoch 116/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4251\n",
            "Epoch 117/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4177\n",
            "Epoch 118/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4091\n",
            "Epoch 119/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3959\n",
            "Epoch 120/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3955\n",
            "Epoch 121/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3998\n",
            "Epoch 122/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3798\n",
            "Epoch 123/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3764\n",
            "Epoch 124/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4319\n",
            "Epoch 125/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.4079\n",
            "Epoch 126/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3701\n",
            "Epoch 127/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3721\n",
            "Epoch 128/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3694\n",
            "Epoch 129/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3565\n",
            "Epoch 130/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3693\n",
            "Epoch 131/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3608\n",
            "Epoch 132/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3571\n",
            "Epoch 133/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3519\n",
            "Epoch 134/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3554\n",
            "Epoch 135/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3648\n",
            "Epoch 136/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3428\n",
            "Epoch 137/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3369\n",
            "Epoch 138/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3271\n",
            "Epoch 139/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3266\n",
            "Epoch 140/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3365\n",
            "Epoch 141/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3186\n",
            "Epoch 142/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3085\n",
            "Epoch 143/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3066\n",
            "Epoch 144/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.3034\n",
            "Epoch 145/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2904\n",
            "Epoch 146/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2995\n",
            "Epoch 147/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.2960\n",
            "Epoch 148/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.2846\n",
            "Epoch 149/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2888\n",
            "Epoch 150/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2792\n",
            "Epoch 151/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.2817\n",
            "Epoch 152/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2940\n",
            "Epoch 153/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2630\n",
            "Epoch 154/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2605\n",
            "Epoch 155/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2651\n",
            "Epoch 156/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2611\n",
            "Epoch 157/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.2794\n",
            "Epoch 158/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2530\n",
            "Epoch 159/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2423\n",
            "Epoch 160/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2526\n",
            "Epoch 161/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2383\n",
            "Epoch 162/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2420\n",
            "Epoch 163/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2471\n",
            "Epoch 164/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2191\n",
            "Epoch 165/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2287\n",
            "Epoch 166/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.2083\n",
            "Epoch 167/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.2206\n",
            "Epoch 168/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.2144\n",
            "Epoch 169/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.2227\n",
            "Epoch 170/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.2277\n",
            "\n",
            "Epoch 00170: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "Epoch 171/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1926\n",
            "Epoch 172/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1899\n",
            "Epoch 173/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1877\n",
            "Epoch 174/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.1857\n",
            "Epoch 175/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.1891\n",
            "Epoch 176/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.1857\n",
            "Epoch 177/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1741\n",
            "Epoch 178/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1967\n",
            "Epoch 179/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.1709\n",
            "Epoch 180/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1814\n",
            "Epoch 181/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1660\n",
            "Epoch 182/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.1618\n",
            "Epoch 183/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1778\n",
            "Epoch 184/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1628\n",
            "Epoch 185/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1603\n",
            "Epoch 186/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1689\n",
            "Epoch 187/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1521\n",
            "Epoch 188/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1622\n",
            "Epoch 189/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1657\n",
            "Epoch 190/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.1439\n",
            "Epoch 191/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.1712\n",
            "Epoch 192/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1424\n",
            "Epoch 193/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.1579\n",
            "Epoch 194/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.1598\n",
            "Epoch 195/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1566\n",
            "Epoch 196/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1479\n",
            "\n",
            "Epoch 00196: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "Epoch 197/200\n",
            "5348/5348 [==============================] - 33s 6ms/step - loss: 3.1505\n",
            "Epoch 198/200\n",
            "5348/5348 [==============================] - 34s 6ms/step - loss: 3.1511\n",
            "Epoch 00198: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzFfaYIXt_Jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #importing necessary modules\n",
        "from __future__ import absolute_import, print_function, unicode_literals, division\n",
        "import os\n",
        "import gc\n",
        "gc.enable()\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "import glob\n",
        "import pickle\n",
        "import numpy\n",
        "from music21 import converter, instrument, note, chord, stream  \n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Dropout, BatchNormalization as BatchNorm, LSTM, Activation \n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
        "from keras.utils import plot_model, np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6Z_OCjqh84J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def generate():\n",
        "    \"\"\" Generate a piano midi file \"\"\"\n",
        "    #load the notes used to train the model\n",
        "    with open('data/notes', 'rb') as filepath:\n",
        "        notes = pickle.load(filepath)\n",
        "\n",
        "    # Get all pitch names\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "    # Get all pitch names\n",
        "    n_vocab = len(set(notes))\n",
        "\n",
        "    network_input, normalized_input = prepare_sequences(notes, pitchnames, n_vocab)\n",
        "    model = create_network(normalized_input, n_vocab)\n",
        "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
        "    create_midi(prediction_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_aaJ-VniGet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sequences(notes, pitchnames, n_vocab):\n",
        "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
        "    # map between notes and integers and back\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    sequence_length = 1000\n",
        "    network_input = []\n",
        "    output = []\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # reshape the input into a format compatible with LSTM layers\n",
        "    normalized_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    # normalize input\n",
        "    normalized_input = normalized_input / float(n_vocab)\n",
        "\n",
        "    return (network_input, normalized_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDQolwJSiLdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def create_network(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(\n",
        "        128,\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "        recurrent_dropout=0.3,\n",
        "        return_sequences=True\n",
        "    ))\n",
        "    model.add(LSTM(64, recurrent_dropout=0.3))\n",
        "    model.add(BatchNorm())\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(32))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNorm())\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(n_vocab))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop')\n",
        "\n",
        "    # Load the weights to each node\n",
        "    model.load_weights('weights.hdf5')\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQxEbSMYiPUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
        "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
        "    # pick a random sequence from the input as a starting point for the prediction\n",
        "    start = numpy.random.randint(0, len(network_input)-1)\n",
        "\n",
        "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    pattern = network_input[start]\n",
        "    prediction_output = []\n",
        "\n",
        "    # generate 500 notes\n",
        "    for note_index in range(500):\n",
        "        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "        prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "        prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "        index = numpy.argmax(prediction)\n",
        "        result = int_to_note[index]\n",
        "        prediction_output.append(result)\n",
        "\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "\n",
        "    return prediction_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb7xbEtOiR8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_midi(prediction_output):\n",
        "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for pattern in prediction_output:\n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    midi_stream.write('midi', fp='test_output.mid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFEAjlWeiXCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    generate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7mwsXIRhHhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}